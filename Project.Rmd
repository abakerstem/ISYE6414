---
title: "Even Newer Project"
author: "Alayna Baker"
date: "2024-04-12"
output: html_document
---

```{r}

# STRUCTURE: 
# 1. data description 
# 2. pre-preprocessing data
# 3. visuals (histograms and boxplots)
# 4. multicollinearity plot
# 5. model fitting
# 6. cook's distance (outliers)
# 7. model comparisons (AIC, BIC, variable significance)
# 8. goodness of fit (overall, pearson, deviance)
# 9. multicollinearity (w/ our final model)
# 10. prediction 
# 11. interpret prediction
# 
# # THINGS TO ADD 
# - COOKS DISTANCE

```


```{r}

############ DATA DESCRIPTION ############

library(dplyr)
setwd("/Users/alaynabaker/Downloads/Regression")
glm_data0 <- read.csv("diabetes_prediction_dataset.csv")
head(glm_data0)
attach(glm_data0)

glm_data0$gender <- ifelse(glm_data0$gender == "Male", 0, ifelse(glm_data0$gender == "Female", 1, 2))
# recoding 
glm_data0$smoking_history <- factor(glm_data0$smoking_history)

# subset data to 40 and older
glm_data0 <- subset(glm_data0, age >= 30)

# taking out smoking_history = "no info"
glm_data0 <- subset(glm_data0, smoking_history != "No Info")

head(glm_data0)
summary(glm_data0)

```

```{r}
# VISUALIZING DATA 

# PLOTTING VARIABLES
library(ggplot2)
library(corrplot)
library(dplyr)

par(mfrow = c(1,2))
glm_data0$diabetes <- factor(glm_data0$diabetes)

ggplot(glm_data0, aes(x = diabetes, y = gender)) + geom_boxplot()
ggplot(glm_data0, aes(x = diabetes, y = age)) + geom_boxplot()
barplot(table(glm_data0$hypertension, glm_data0$diabetes), legend.text = TRUE, beside = TRUE, xlab = "Diabetes", ylab = "Count of Hypertension" )
barplot(table(glm_data0$heart_disease, glm_data0$diabetes), legend.text = TRUE, beside = TRUE, xlab = "Diabetes", ylab = "Count of Heart Disease" )
ggplot(glm_data0, aes(diabetes,..count..)) + geom_bar(aes(fill = smoking_history), position = "dodge")
ggplot(glm_data0, aes(x = diabetes, y = bmi)) + geom_boxplot()
ggplot(glm_data0, aes(x = diabetes, y = HbA1c_level)) + geom_boxplot()
ggplot(glm_data0, aes(x = diabetes, y = blood_glucose_level)) + geom_boxplot()

```

```{r}
# MULTICOLLINEARITY OR OUTLIERS? 

cordata <- subset(glm_other_data, select = -c(smoking_history, diabetes))
corrplot(cor(cordata), method="square")
round(cor(cordata),2)
```

```{r}
# first logistic regression model - all variables 
glm_model0 <- glm(diabetes ~., data=glm_data0, family=binomial)
summary(glm_model0)

step_model0 <- step(glm(diabetes ~., data=glm_data0, family=binomial), direction = "both")

step_model1 <- step(glm(diabetes ~., data=glm_data0, family=binomial), direction = "forward")

step_model2 <- step(glm(diabetes ~., data=glm_data0, family=binomial), direction = "backward")

# taking out smoking_history = "ever and former"
glm_data1 <- subset(glm_data0, smoking_history != "ever" & smoking_history != "former")

# second logistic regression model - reduced
glm_model1 <- glm(diabetes ~., data=glm_data1, family=binomial)
summary(glm_model1)

step_model3 <- step(glm(diabetes ~., data=glm_data1, family=binomial), direction = "both")

step_model4 <- step(glm(diabetes ~., data=glm_data1, family=binomial), direction = "forward")

step_model5 <- step(glm(diabetes ~., data=glm_data1, family=binomial), direction = "backward")

# taking out smoking history 
glm_data2 <- subset(glm_data1, select = -c(smoking_history))

glm_model2 <- glm(diabetes ~., data=glm_data2, family=binomial)
summary(glm_model2)

BIC(glm_model0, glm_model1, glm_model2)

# cooks distance? EVERYTHING OK

cook = cooks.distance(glm_model2)
plot(cook, type="h", lwd=3, col="red", vlab = "Cook's Distance")

# MULTICOLLINEARITY 

library(car)
max(10, 1/(1-summary(glm_model2)$r.squared))
vif(glm_model2)
```

```{r}
# GOODNESS OF FIT 

# hypothesis test - is the regression overall significant? 
# test for significance on glm_model2 = 0
1 - pchisq(glm_model2$null.deviance - glm_model2$deviance, glm_model2$df.null - glm_model2$df.residual)

# hypothesis tests for GOF 

# deviance test for GOF
c(deviance(glm_model2), 1 - pchisq(deviance(glm_model2),df.residual(glm_model2)))

# pearson residuals gof
pearres2 = residuals(glm_model2, type="pearson")
pearson_tval = sum(pearres2^2)
c(pearson_tval, 1 - pchisq(pearson_tval, 37419))
```

```{r}
# SPLITTING DATA INTO TRAINING TEST 
set.seed(123)
sample <- sample(c(TRUE,FALSE), nrow(glm_data2), replace=TRUE, prob=c(0.8, 0.2))
train <- glm_data2[sample,]
test <- glm_data2[!sample,]

# TRAINING using third linear regression model - cooksD0: R^2 = 40.55
train_model <-  glm(diabetes ~ ., data=train, family=binomial)
summary(train_model)

glm_pred <- predict(train_model, test, type="response")

df <- data.frame(pred = glm_pred,
                actuals = test$diabetes)

df

# 12.83% of people with diabetes in original dataset 
# 10.01% of people with diabetes in OUR prediction
# diab <- subset(glm_data2, diabetes == 1)
# nrow(diab)
# nrow(glm_data2)
# 
# over_50 <- subset(df, pred > 0.5)
# nrow(over_50)

df$actual_pred <- ifelse(df$pred >= 0.5, 1, 0)
head(df)
```

```{r}
# >= 0.5: 
# Accuracy: 0.9433554 
# Precision: 0.8815612 
# Recall: 0.6643002 
# F1 Score: 0.7576634

df$actual_pred <- ifelse(df$pred >= 0.5, 1, 0)
head(df)

conf_matrix <- table(df$actuals, df$actual_pred)

# Calculate accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Accuracy:", accuracy, "\n")

# Calculate precision
precision <- conf_matrix[2, 2] / sum(conf_matrix[, 2])
cat("Precision:", precision, "\n")

# Calculate recall (sensitivity)
recall <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
cat("Recall:", recall, "\n")

# Calculate F1-score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")
```











```
